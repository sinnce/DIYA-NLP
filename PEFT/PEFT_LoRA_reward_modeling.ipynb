{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PEFTë¥¼ í™œìš©í•œ LLM Fine-tuningì‹œí‚¤ê¸° (with TransformersğŸ¤— & bitsandbytes)\n",
        "ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” 8ë¹„íŠ¸ë¡œ í° ëª¨ë¸ì„ ë¡œë“œí•˜ê¸° ìœ„í•´ ìµœì‹  peft ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ LLMì„ íŒŒì¸íŠœë‹í•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£° ê²ƒì…ë‹ˆë‹¤. íŒŒì¸íŠœë‹ ë°©ë²•ì€ ì „ì²´ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•˜ëŠ” ëŒ€ì‹  ì–´ëŒ‘í„°ë¥¼ íŠœë‹ì‹œí‚¤ê³  ëª¨ë¸ ë‚´ë¶€ì— ì ì ˆí•˜ê²Œ ë¡œë“œí•˜ê¸°ë§Œ í•˜ë©´ ë˜ëŠ” \"LoRA\"ë¼ëŠ” ìµœì‹  ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•œ í›„ì—ëŠ” í—ˆê¹…í˜ì´ìŠ¤ğŸ¤— í—ˆë¸Œì—ì„œ ì–´ëŒ‘í„°ë¥¼ ê³µìœ í•˜ì—¬ ë§¤ìš° ì‰½ê²Œ ë¡œë“œí•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. í™•ì¸í•´ë³´ì‹œì£ !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xk5l_r7RZx3A"
      },
      "outputs": [],
      "source": [
        "# í›ˆë ¨ì— í•„ìš”í•œ íŒ¨í‚¤ì§€ë“¤ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤ / ì§€ê¸ˆ ë²„ì ¼ì€ êµ¬ê¸€ ì½”ë© ì‚¬ìš©ìë¥¼ ëŒ€ìƒìœ¼ë¡œ ì„¤ì¹˜ë¥¼ ê°€ì´ë“œ í•©ë‹ˆë‹¤.\n",
        "!pip install transformers[torch] peft datasets sentencepiece evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uJl3pL7bDvsG"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "import sentencepiece\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RLHFì— ì“°ì´ëŠ” Reward Modelì„ íŒŒì¸íŠœë‹ ì‹œí‚¤ê¸° ìœ„í•œ ë°ì´í„°ì™€ Base Model ì¤€ë¹„í•˜ê¸°\n",
        "\n",
        "ë‘ ë¬¸ì¥ì„ ë¹„êµí•˜ì—¬ ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œì§€ ë˜ëŠ” ì–¼ë§ˆë‚˜ ì˜ ë”°ëëŠ”ì§€(Align) ì˜ˆì¸¡í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ ë§Œë“¤ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì€ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ 2ê°€ì§€ featureë¥¼ inputìœ¼ë¡œ ë°›ëŠ” LLMì„ íŒŒì¸íŠœë‹ ì‹œí‚¤ê² ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qu_oudCmHYUL"
      },
      "outputs": [],
      "source": [
        "train_dataset = load_dataset(\"fiveflow/cot_ranking\", split=\"train\")\n",
        "eval_dataset = load_dataset(\"fiveflow/cot_ranking\", split=\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë°ì´í„°ë¥¼ íŒŒì•…í•˜ë‹ˆ questionê³¼ j,k ì‘ë‹µì´ ìˆìŠµë‹ˆë‹¤. ìì„¸í•œ ì„¤ëª…ì„ ë“œë¦¬ì§€ ì•Šì•˜ì§€ë§Œ ì§€ê¸ˆ ì‚¬ìš©í•˜ëŠ” ë°ì´í„°ì…‹ì€ ì €í¬ê°€ ë§Œë“  ë°ì´í„°ì…‹ì´ê³ , LLMì´ ìŠ¤ìŠ¤ë¡œ Jì‘ë‹µì´ Kì‘ë‹µë³´ë‹¤ ë” ì¢‹ë‹¤ê³  íŒë‹¨í•˜ì—¬ êµ¬ì¶•í•œ ë°ì´í„°ì…ë‹ˆë‹¤.\n",
        "# ë”°ë¼ì„œ Jì‘ë‹µì€ (LLM ê¸°ì¤€) í•­ìƒ Kì‘ë‹µë³´ë‹¤ ë” ë‚˜ì€ Preferenceë¥¼ ê°–ìŠµë‹ˆë‹¤.\n",
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFNJuvFHHsq_",
        "outputId": "c1aa16ef-d54a-43fa-98d8-76ce72e2a6ed"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at facebook/opt-iml-max-1.3b and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "rank_model, tokenizer = AutoModelForSequenceClassification.from_pretrained(\"facebook/opt-iml-max-1.3b\", num_labels=1),AutoTokenizer.from_pretrained(\"facebook/opt-iml-max-1.3b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RlCrWUfRIS4U"
      },
      "outputs": [],
      "source": [
        "# ë°ì´í„°ì…‹ì˜ ì „ì²˜ë¦¬ ê³¼ì •ì…ë‹ˆë‹¤. questionì— ëŒ€í•œ jì‘ë‹µê³¼ kì‘ë‹µì„ ë¹„êµí•˜ê³  ì‹¶ê¸° ë•Œë¬¸ì— (question + jì‘ë‹µ, question + kì‘ë‹µ) ìŒìœ¼ë¡œ ë°ì´í„°ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.\n",
        "\"\"\"\n",
        "num_procì€ ì¸í”„ë¼ í™˜ê²½ì— ë”°ë¼ 1ë¡œ ë‘ì…”ë„ ì¢‹ìŠµë‹ˆë‹¤.\n",
        "original columnsë¥¼ ì œê±°í•˜ëŠ” ì´ìœ ëŠ” ì“¸ë°ì—†ëŠ” ë°ì´í„°ê°€ ë©”ëª¨ë¦¬ ì°¨ì§€í•˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.\n",
        "max_length ë˜í•œ ì¸í”„ë¼ í™˜ê²½ì— ë”°ë¼ ë³€ê²½í•˜ì…”ë„ ì¢‹ìŠµë‹ˆë‹¤. ë‹¨, questionê³¼ ì‘ë‹µì„ í•©ì¹œê²ƒì´ í•œ Inputì´ê¸° ë•Œë¬¸ì— ë°ì´í„°ë¥¼ í™•ì¸í•˜ì—¬ ì§¤ë ¤ë„ ê´œì°®ë‹¤ê³  ìƒê°í–ˆì„ ê²½ìš° ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n",
        "\"\"\"\n",
        "\n",
        "num_proc = 24\n",
        "original_columns = train_dataset.column_names\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    new_examples = {\n",
        "            \"input_ids_j\": [],\n",
        "            \"attention_mask_j\": [],\n",
        "            \"input_ids_k\": [],\n",
        "            \"attention_mask_k\": [],\n",
        "        }\n",
        "    for question, response_j, response_k in zip(examples[\"question\"], examples[\"response_j\"], examples[\"response_k\"]):\n",
        "        tokenized_j = tokenizer(\"Question: \" + question + \"\\n\\nAnswer: \" + response_j)\n",
        "        tokenized_k = tokenizer(\"Question: \" + question + \"\\n\\nAnswer: \" + response_k)\n",
        "\n",
        "        new_examples[\"input_ids_j\"].append(tokenized_j[\"input_ids\"])\n",
        "        new_examples[\"attention_mask_j\"].append(tokenized_j[\"attention_mask\"])\n",
        "        new_examples[\"input_ids_k\"].append(tokenized_k[\"input_ids\"])\n",
        "        new_examples[\"attention_mask_k\"].append(tokenized_k[\"attention_mask\"])\n",
        "\n",
        "    return new_examples\n",
        "\n",
        "\n",
        "max_length = 1024\n",
        "train_dataset = train_dataset.map(\n",
        "    preprocess_function, batched=True, num_proc=num_proc, remove_columns=original_columns\n",
        "    )\n",
        "train_dataset = train_dataset.filter(lambda x: len(x[\"input_ids_j\"]) <= max_length and len(x[\"input_ids_k\"]) <= max_length)\n",
        "\n",
        "eval_dataset = eval_dataset.map(preprocess_function, batched=True, num_proc=num_proc, remove_columns=original_columns)\n",
        "eval_dataset = eval_dataset.filter(lambda x: len(x[\"input_ids_j\"]) <= max_length and len(x[\"input_ids_k\"]) <= max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VjT9ws0uIS2t"
      },
      "outputs": [],
      "source": [
        "# ë‹¤ìŒì€ padding ì „ëµì…ë‹ˆë‹¤. ëª¨ë“  ë°ì´í„°ì¤‘ ê°€ì¥ ê¸´ ë¬¸ì¥ì— ëŒ€í•´ paddingì„ í•˜ê²Œ ëœë‹¤ë©´ ì—°ì‚°ì´ ì“¸ë° ì—†ì´ ê¸¸ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
        "# ë”°ë¼ì„œ í•œ batchì•ˆì—ì„œ ê°€ì¥ ê¸´ ë¬¸ì¥ì— ëŒ€í•´ paddingì„ í•œë‹¤ë©´ ë” íš¨ê³¼ì ì¼ ê²ƒ ì…ë‹ˆë‹¤.\n",
        "\n",
        "from transformers import PreTrainedTokenizerBase\n",
        "from transformers.utils import PaddingStrategy\n",
        "import evaluate\n",
        "\n",
        "@dataclass\n",
        "class RewardDataCollatorWithPadding:\n",
        "    tokenizer: PreTrainedTokenizerBase\n",
        "    padding: Union[bool, str, PaddingStrategy] = True\n",
        "    max_length: Optional[int] = None\n",
        "    pad_to_multiple_of: Optional[int] = None\n",
        "    return_tensors: str = \"pt\"\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "        features_j = []\n",
        "        features_k = []\n",
        "        for feature in features:\n",
        "            features_j.append(\n",
        "                {\n",
        "                    \"input_ids\": feature[\"input_ids_j\"],\n",
        "                    \"attention_mask\": feature[\"attention_mask_j\"],\n",
        "                }\n",
        "            )\n",
        "            features_k.append(\n",
        "                {\n",
        "                    \"input_ids\": feature[\"input_ids_k\"],\n",
        "                    \"attention_mask\": feature[\"attention_mask_k\"],\n",
        "                }\n",
        "            )\n",
        "        batch_j = self.tokenizer.pad(\n",
        "            features_j,\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "            return_tensors=self.return_tensors,\n",
        "        )\n",
        "        batch_k = self.tokenizer.pad(\n",
        "            features_k,\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "            return_tensors=self.return_tensors,\n",
        "        )\n",
        "        batch = {\n",
        "            \"input_ids_j\": batch_j[\"input_ids\"],\n",
        "            \"attention_mask_j\": batch_j[\"attention_mask\"],\n",
        "            \"input_ids_k\": batch_k[\"input_ids\"],\n",
        "            \"attention_mask_k\": batch_k[\"attention_mask\"],\n",
        "            \"return_loss\": True,\n",
        "        }\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë©”íŠ¸ë¦­ ì •ì˜ì…ë‹ˆë‹¤.\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, _ = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=0)\n",
        "    labels = np.zeros(predictions.shape, dtype=int)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SNnIShEXIS0c"
      },
      "outputs": [],
      "source": [
        "# í—ˆê¹…í˜ì´ìŠ¤ì—ì„œ ì œê³µí•˜ëŠ” trainer ì¤‘ loss ë¶€ë¶„ì„ ìˆ˜ì •í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ìœ ëŠ” jì‘ë‹µê³¼ kì‘ë‹µì„ ë¹„êµí•´ì•¼í•˜ëŠ”ë° AutoModelForSequenceClassificationìœ¼ë¡œ ë¶ˆëŸ¬ì˜¨ ëª¨ë¸ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê¸°ì—ëŠ” ë¶€ì ì ˆí•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
        "# ë”°ë¼ì„œ Inputì„ ë‘ë²ˆ ë°›ê³  ê·¸ logitê°’ì„ ë¹„êµí•˜ì—¬ j ì‘ë‹µì— ë” ê°€ê¹ê²Œ í›ˆë ¨í•˜ë„ë¡ RLHFì—ì„œ ì œì•ˆí•œ negative log sigmoidí•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì´ í›ˆë ¨í•˜ë„ë¡ ì„ ì–¸í•©ë‹ˆë‹¤.\n",
        "\n",
        "from transformers import Trainer\n",
        "class RewardTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        rewards_j = model(input_ids=inputs[\"input_ids_j\"], attention_mask=inputs[\"attention_mask_j\"])[0]\n",
        "        rewards_k = model(input_ids=inputs[\"input_ids_k\"], attention_mask=inputs[\"attention_mask_k\"])[0]\n",
        "        loss = -nn.functional.logsigmoid(rewards_j - rewards_k).mean()\n",
        "        if return_outputs:\n",
        "            return loss, {\"rewards_j\": rewards_j, \"rewards_k\": rewards_k}\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PEFTë¥¼ í™œìš©í•˜ì—¬ í›ˆë ¨ ëª¨ë¸ ì„ ì–¸í•˜ê¸°\n",
        "\n",
        "ì§€ê¸ˆê¹Œì§€ëŠ” RLHFì˜ Reward Modelì„ í›ˆë ¨ì‹œí‚¤ê¸°ê¹Œì§€ ê³¼ì •ì„ ì‚´í´ë³´ì•˜ì§€ë§Œ, êµ¬ê¸€ ì½”ë©ì„ ì‚¬ìš©í•˜ê²Œ ë˜ë©´ opt iml 1.3bëª¨ë¸ë¡œë„ ë²…ì°° ê°€ëŠ¥ì„±ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤. Reward modelì´ë‹ˆ ì–´ëŠì •ë„ ì„±ëŠ¥ë§Œ ë‚˜ì˜¤ë©´ ëœë‹¤ ìƒê°í•  ìˆ˜ ìˆì§€ë§Œ, PPOì— ì˜í•´ í›ˆë ¨í•˜ëŠ” ê³¼ì •ì—ì„œ reward modelì˜ ê²°ê³¼ì— ì¢Œì§€ìš°ì§€ ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ Reward Modelingì˜ ì •í™•ë„ëŠ” ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ê°€ëŠ¥í•œ ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë¥¼ ëŠ˜ë ¤ í›ˆë ¨ì„ ê½‰ê½‰ ì±„ìš°ê¸° ìœ„í•´ LoRAë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ì„ ì‹œì¼œë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wqU8XKKH5UM",
        "outputId": "a4b9c43a-089c-48e3-e4a0-096f4753ac11"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 790,528 || all params: 1,316,548,608 || trainable%: 0.06004548523285515\n"
          ]
        }
      ],
      "source": [
        "# ê¸°ì¡´ì˜ AutoModelForSequenceClassificationìœ¼ë¡œ ë¶ˆëŸ¬ì˜¨ ëª¨ë¸ì— LoRA ì–´ëŒ‘í„°ë¥¼ ì¶”ê°€ë§Œ í•˜ë©´ ë˜ê¸° ë•Œë¬¸ì— ë‹¤ìŒê³¼ ê°™ì€ configë¥¼ ì§€ì •í•˜ê³  get_peft_modelì„ ì‚¬ìš©í•˜ì—¬ ìƒˆë¡œìš´ ëª¨ë¸ ì„ ì–¸ë§Œ í•´ì£¼ë©´ ëì…ë‹ˆë‹¤.\n",
        "\n",
        "\"\"\"\n",
        "task_typeì€ ë‹¤ì–‘í•©ë‹ˆë‹¤. [CAUSAL_LM, FEATURE_EXTRACTION, QUESTION_ANS, SEQ_2_SEQ_LM, SEQ_CLS, TOKEN_CLS]ê°€ ìˆìŠµë‹ˆë‹¤. ì €í¬ëŠ” j,kì‘ë‹µì„ ë¹„êµí•˜ì—¬ scoreë¥¼ ì–»ê³  ì‹¶ê¸° ë•Œë¬¸ì— SEQ_CLSë¥¼ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤.\n",
        "r, lora_alpha, lora_dropoutì€ pdfì— ìˆëŠ” ë…¼ë¬¸ ì„¤ëª…ìœ¼ë¡œ ì„¤ëª…ì„ ëŒ€ì²´í•˜ê² ìŠµë‹ˆë‹¤.\n",
        "\"\"\"\n",
        "\n",
        "from peft import prepare_model_for_int8_training\n",
        "model = prepare_model_for_int8_training(rank_model)\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "    inference_mode=False,\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\"j_proj\", \"k_proj\"]\n",
        ")\n",
        "lora_model = get_peft_model(model, peft_config)\n",
        "lora_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pM-FE4_5INYz",
        "outputId": "d066fa97-b411-4af0-c131-7395963bd00a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OPTForSequenceClassification(\n",
              "  (model): OPTModel(\n",
              "    (decoder): OPTDecoder(\n",
              "      (embed_tokens): Embedding(50272, 2048, padding_idx=1)\n",
              "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 2048)\n",
              "      (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (layers): ModuleList(\n",
              "        (0-23): 24 x OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(\n",
              "              in_features=2048, out_features=2048, bias=True\n",
              "              (lora_dropout): ModuleDict(\n",
              "                (default): Dropout(p=0.05, inplace=False)\n",
              "              )\n",
              "              (lora_A): ModuleDict(\n",
              "                (default): Linear(in_features=2048, out_features=8, bias=False)\n",
              "              )\n",
              "              (lora_B): ModuleDict(\n",
              "                (default): Linear(in_features=8, out_features=2048, bias=False)\n",
              "              )\n",
              "              (lora_embedding_A): ParameterDict()\n",
              "              (lora_embedding_B): ParameterDict()\n",
              "            )\n",
              "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (score): ModulesToSaveWrapper(\n",
              "    (original_module): Linear(in_features=2048, out_features=1, bias=False)\n",
              "    (modules_to_save): ModuleDict(\n",
              "      (default): Linear(in_features=2048, out_features=1, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rank_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whIlbg_3J9A5",
        "outputId": "77b8f5b2-c596-4ad8-a3b5-7be84dbd7026"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PeftModelForSequenceClassification(\n",
              "  (base_model): LoraModel(\n",
              "    (model): OPTForSequenceClassification(\n",
              "      (model): OPTModel(\n",
              "        (decoder): OPTDecoder(\n",
              "          (embed_tokens): Embedding(50272, 2048, padding_idx=1)\n",
              "          (embed_positions): OPTLearnedPositionalEmbedding(2050, 2048)\n",
              "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "          (layers): ModuleList(\n",
              "            (0-23): 24 x OPTDecoderLayer(\n",
              "              (self_attn): OPTAttention(\n",
              "                (k_proj): Linear(\n",
              "                  in_features=2048, out_features=2048, bias=True\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Dropout(p=0.05, inplace=False)\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=2048, out_features=8, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=8, out_features=2048, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                )\n",
              "                (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "                (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "                (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "              )\n",
              "              (activation_fn): ReLU()\n",
              "              (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "              (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "              (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "              (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (score): ModulesToSaveWrapper(\n",
              "        (original_module): Linear(in_features=2048, out_features=1, bias=False)\n",
              "        (modules_to_save): ModuleDict(\n",
              "          (default): Linear(in_features=2048, out_features=1, bias=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lora_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CUCox1dSJkv1"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='/content/drive/MyDrive/Colab Notebooks/project/rm_opt',\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.001,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_total_limit = 3,\n",
        "    eval_steps=500,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    gradient_accumulation_steps=1,\n",
        "    gradient_checkpointing=False,\n",
        "    deepspeed=None,\n",
        "    local_rank=-1,\n",
        "    remove_unused_columns=False,\n",
        "    label_names=[],\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=10,\n",
        "    optim=\"adamw_hf\",\n",
        "    lr_scheduler_type=\"linear\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7zJ5acU0JcFt"
      },
      "outputs": [],
      "source": [
        "trainer = RewardTrainer(\n",
        "    model=lora_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=RewardDataCollatorWithPadding(tokenizer=tokenizer, max_length=512),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "rlTroeLAJjd1",
        "outputId": "f8750316-0139-4203-b733-9ff8ba79f57e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2640: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='49' max='135660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [    49/135660 00:26 < 21:22:21, 1.76 it/s, Epoch 0.00/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-3435b262f1ae>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1591\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1592\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1870\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1871\u001b[0m                 \u001b[0mtotal_batched_samples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1872\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrng_to_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                     \u001b[0mcurrent_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                 \u001b[0mnext_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mcurrent_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_profile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/profiler.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDisableTorchFunctionSubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_function_exit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RecordFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_function_exit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_Sc2JXoJn95"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
